# Pronunciation Assessment Application

## Overview

This application provides a platform for pronunciation assessment. Users can upload audio recordings of their speech. The system transcribes the audio, performs a word-by-word phonetic analysis by comparing expected phonemes (derived from transcribed text) with predicted phonemes (from audio segments), classifies pronunciation accuracy using a machine learning model, and can generate audio feedback for mispronounced words.

The application consists of:

* A **Backend API** built with FastAPI, handling all processing.

* A **Frontend UI** built with Streamlit for user interaction.

## Directory Structure (Assumed)


pronunciation-correction/
├── CrisperWhisper_cloned/      # Cloned nyrahealth/CrisperWhisper repo
│   └── transcribe.py           # User's modified version is copied here
├── backend/
│   ├── app/
│   │   ├── core/
│   │   ├── external/             # (Not strictly used if transcribe.py is in CrisperWhisper_cloned)
│   │   ├── models_api/
│   │   ├── services/
│   │   └── assets/
│   │       ├── processed_with_alignments.csv # User provided
│   │       └── phoneme_classifier.joblib     # Generated by training script
│   ├── Dockerfile                # (Example provided, not fully set up in this CLI guide)
│   └── requirements.txt
├── frontend/
│   ├── app_streamlit.py
│   └── requirements.txt
├── scripts/
│   └── train_classifier.py
├── venv_scripts/               # Temporary venv for training script
├── venv_backend/               # Venv for backend
├── venv_frontend/              # Venv for frontend
├── .gitignore
└── README.md                   # This file


## Prerequisites

Ensure the following are installed on your **macOS** ARM (Apple Silicon M1/M2/M3) **system**:

1. **Homebrew:** Package manager for macOS. If not installed, get it from <https://brew.sh/>.

2. **Python 3 (ARM64 Native):**

   * Install via Homebrew: `brew install python`

   * Verify it's ARM: `python3 -c "import platform; print(platform.machine())"` (should output `arm64`).

3. **Git:** For cloning repositories. `brew install git` or it might be pre-installed.

4. **Xcode Command Line Tools:** Needed for compiling some Python packages.


xcode-select --install


5. **System Libraries:**

* **FFmpeg** (for `pydub`):

  ```
  brew install ffmpeg
  
  ```

* **espeak-ng** (for `phonemizer`):

  ```
  brew install espeak-ng
  
  ```

## Setup Instructions

**All commands below assume you are in the root directory of your `pronunciation_app` project (e.g., `/path/to/pronunciation_app/`).**

**1.** Clone the **`CrisperWhisper` Repository**
This repository is needed for its underlying code and dependencies.


Ensure you are in the root of your pronunciation_app project
git clone https://github.com/nyrahealth/CrisperWhisper.git CrisperWhisper_cloned


**2. Prepare User-Specific Files**

a.  **Copy Your Modified `transcribe.py`:**
Replace the `transcribe.py` within the `CrisperWhisper_cloned` directory with your modified version.
`bash #` Replace` /path/to/your/MODIFIED_transcribe.py with the actual path to your file cp /path/to/your/MODIFIED_transcribe.py ./CrisperWhisper_cloned/transcribe.py `

b.  **Place** Training Data CSV **(`processed_with_alignments.csv`):**
This file is needed to train the pronunciation classifier.
`bash mkdir -p backend/app/assets/ # Replace /path/to/your/processed_with_alignments.csv with the actual path cp /path/to/your/processed_with_alignments.csv backend/app/assets/processed_with_alignments.csv `

**3. Train the Pronunciation Classifier**
This step creates the `phoneme_classifier.joblib` model file used by the backend.


Ensure you are in the root directory of your pronunciation_app project
echo "Current directory: $(pwd)"
echo "Ensure this is the root of your 'pronunciation_app' project."

Create and activate a temporary virtual environment for the training script
python3 -m venv venv_scripts
source venv_scripts/bin/activate

Install dependencies for the training script (macOS ARM)
echo "Installing dependencies for the training script..."
pip install torch torchvision torchaudio pandas scikit-learn joblib Levenshtein

Run the training script
echo "Running classifier training script..."
python scripts/train_classifier.py

Deactivate the virtual environment
deactivate
echo "Classifier training complete. Model (phoneme_classifier.joblib) is in backend/app/assets/"

Optional: rm -rf venv_scripts

## Running the Application

You will need **two separate terminal windows**: one for the backend and one for the frontend.

**1. Start the Backend Server (FastAPI)**

* Open your first terminal window.

* Navigate to the root of your `pronunciation_app` project.


In Terminal 1:
cd backend/ # Navigate into the backend directory
python3 -m venv venv_backend
source venv_backend/bin/activate

echo "Installing CrisperWhisper dependencies (from ../CrisperWhisper_cloned/requirements.txt)..."
pip install -r ../CrisperWhisper_cloned/requirements.txt

echo "Installing main backend dependencies (from ./requirements.txt)..."
pip install -r requirements.txt

echo "Starting backend server on http://localhost:8000 ..."

PyTorch should automatically use 'mps' device on macOS ARM.
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload


* Leave this terminal running. The backend server is now active.

* On the first API call, models from Hugging Face (e.g., `nyrahealth/CrisperWhisper`, `facebook/wav2vec2-lv-60-espeak-cv-ft`) will be downloaded by the respective services. This requires an internet connection and may take time.

* You can check the API docs at `http://localhost:8000/docs`.

**2. Start the Frontend Application (Streamlit)**

* Open a **new, separate terminal window**.

* Navigate to the root of your `pronunciation_app` project.


In Terminal 2 (New Window):
cd frontend/ # Navigate into the frontend directory
python3 -m venv venv_frontend
source venv_frontend/bin/activate

pip install -r requirements.txt

echo "Starting frontend application..."
streamlit run app_streamlit.py


* Streamlit will provide a "Local URL" (usually `http://localhost:8501`). Open this URL in your web browser.

## Accessing the Application

* **Backend API Docs:** `http://localhost:8000/docs`

* **Frontend UI:** `http://localhost:8501` (or the URL provided by Streamlit)

## Stopping the Application

1. In each of the two terminal windows (one for the backend, one for the frontend), press `Ctrl+C` to stop the server/application.

2. To deactivate the virtual environments (optional, as closing the terminal usually handles this):


deactivate


## Troubleshooting / Notes

* **Model Downloads:** The first time the backend processes an audio file, it will download required models from Hugging Face. This can take several minutes depending on your internet speed and model sizes. Subsequent runs will use cached models.

* **macOS ARM & PyTorch:** Ensure your Python and PyTorch installations are native ARM64 for best performance. The `pip install` commands should generally pick up compatible versions.

* **Custom `transformers` from `CrisperWhisper_cloned`:** The installation of `CrisperWhisper_cloned/requirements.txt` is crucial as it often contains a line like `transformers @ git+https://github.com/nyrahealth/transformers.git@crisper_whisper`. If this fails, ensure Xcode Command Line Tools are up to date. If issues persist, you might need to investigate ARM64 compatibility of that specific fork or any C/C++ extensions it might build.

* **Temporary Files:**

* The backend creates temporary session directories (e.g., under `/var/folders/.../T/pronunciation_sessions_*` on macOS) to store segmented word audio clips.

* For development, the automatic cleanup of these session directories in `backend/app/main.py` (the line `background_tasks.add_task(processing_service.cleanup_session_files, session_id)`) has been commented out in previous discussions to allow feedback requests to work. This means files will accumulate. Manually delete these temporary directories if they grow too large. For production, a proper cleanup strategy is needed.

* **Python Version:** This setup was discussed in the context of Python 3.11/3.13. Ensure your `python3` command points to a compatible version.

* **Paths in Scripts:** The scripts and service modules assume a certain project structure (e.g., `CrisperWhisper_cloned` at the project root, `assets` within `backend/app/`). If you change these, update the path variables in the respective Python files (e.g., `CRISPER_WHISPER_CLONED_DIR` in `processing_service.py`, paths in `config.py`).
